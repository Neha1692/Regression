{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgf-K6vTu_rX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1 What is Simple Linear Regression?\n",
        "\n",
        "Ans-Simple Linear Regression is a statistical technique that models the relationship between a dependent variable (Y) and a single independent variable (X) using a straight line:\n",
        "Y = mX + c, where:\n",
        "\n",
        "m is the slope (effect of X on Y)\n",
        "\n",
        "c is the intercept (Y when X = 0)\n",
        "\n",
        "Q2 What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Ans-The key assumptions of Simple Linear Regression include a linear relationship between the variables, independence of errors, homoscedasticity (constant variance of errors), and normality of errors.\n",
        "\n",
        "Q3 - What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "Ans- In the equation Y = mX + c, the coefficient m represents the slope or gradient of the line.\n",
        "\n",
        "Q4 - What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        "Ans-In the equation y = mx + c, the intercept 'c' represents the y-intercept.\n",
        "\n",
        "Q5 How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "Ans-The slope m is calculated as:\n",
        "\n",
        "ùëö\n",
        "=\n",
        "‚àë\n",
        "(\n",
        "ùëã\n",
        "ùëñ\n",
        "‚àí\n",
        "ùëã\n",
        "Àâ\n",
        ")\n",
        "(\n",
        "ùëå\n",
        "ùëñ\n",
        "‚àí\n",
        "ùëå\n",
        "Àâ\n",
        ")\n",
        "‚àë\n",
        "(\n",
        "ùëã\n",
        "ùëñ\n",
        "‚àí\n",
        "ùëã\n",
        "Àâ\n",
        ")\n",
        "2\n",
        "\n",
        "It‚Äôs the covariance of X and Y divided by the variance of X.\n",
        "\n",
        "Q6  What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "Ans-To minimize the sum of squared differences between the observed values and the predicted values (residuals). This gives the best-fitting line.\n",
        "\n",
        "Q7 How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression?\n",
        "\n",
        "Ans R¬≤ measures the proportion of variance in Y explained by X.\n",
        "\n",
        "R¬≤ = 1: Perfect fit\n",
        "\n",
        "R¬≤ = 0: X explains none of the variance in Y\n",
        "\n",
        "Q8 What is Multiple Linear Regression?\n",
        "\n",
        "Ans-Multiple Linear Regression (MLR) is a statistical method to model the relationship between a dependent variable (Y) and two or more independent variables (X‚ÇÅ, X‚ÇÇ, ..., X‚Çô).\n",
        "\n",
        "Q9 What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "Ans-Simple Linear Regression involves one independent variable.\n",
        "Multiple Linear Regression involves two or more independent variables to predict the outcome.\n",
        "\n",
        "Q10 What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "Ans-Linearity: Linear relationship between predictors and the outcome.\n",
        "\n",
        "Independence: Observations are independent.\n",
        "\n",
        "Homoscedasticity: Constant variance of residuals.\n",
        "\n",
        "Normality: Residuals are normally distributed.\n",
        "\n",
        "No multicollinearity: Predictors are not highly correlated with each other.\n",
        "\n",
        "Q11 - What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Ans-Heteroscedasticity means the residuals have non-constant variance across the range of predicted values.\n",
        "\n",
        "üö´ Why it‚Äôs a problem:\n",
        "Leads to inefficient estimates.\n",
        "\n",
        "Can make hypothesis tests unreliable (like t-tests for coefficients).\n",
        "\n",
        "Q12 - How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "Ans-To reduce multicollinearity:\n",
        "\n",
        "Remove or combine correlated features.\n",
        "\n",
        "Use dimensionality reduction (like PCA).\n",
        "\n",
        "Apply regularization (like Ridge or Lasso regression).\n",
        "\n",
        "Q13  What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "Ans-One-Hot Encoding: Converts categories into binary vectors.\n",
        "\n",
        "Label Encoding: Assigns a unique integer to each category (works best for ordinal data).\n",
        "\n",
        "Target Encoding: Replaces categories with the mean of the target variable (use with caution to avoid leakage).\n",
        "\n",
        "\n",
        "Q14 What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Ans-Interaction terms capture the combined effect of two or more predictors on the target.\n",
        "\n",
        "Q15 How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "Ans-In Simple Linear Regression, the intercept (c) is the value of Y when X = 0.\n",
        "\n",
        "In Multiple Linear Regression, the intercept (b‚ÇÄ) is the predicted value of Y when all independent variables (X‚ÇÅ, X‚ÇÇ, ..., X‚Çô) are 0 ‚Äî which may not be meaningful or even realistic depending on the data.\n",
        "\n",
        "Q16 What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "Ans-Each slope coefficient (b·µ¢) shows the change in Y for a one-unit increase in X·µ¢, assuming all other variables are held constant.\n",
        "\n",
        "A positive slope means a direct relationship; negative means an inverse relationship.\n",
        "\n",
        "Q17 - How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "Ans-The intercept (b‚ÇÄ) provides the baseline value of the target variable when all predictors are zero.\n",
        "\n",
        "It helps define the starting point for prediction but may not always have practical meaning, especially if zero isn't a valid input for the predictors.\n",
        "\n",
        "Q18  What are the limitations of using R¬≤ as a sole measure of model performance?\n",
        "\n",
        "Ans-R¬≤ does not penalize overfitting.\n",
        "\n",
        "It always increases when more predictors are added‚Äîeven if they‚Äôre irrelevant.\n",
        "\n",
        "It doesn‚Äôt indicate if predictors are statistically significant.\n",
        "\n",
        "Doesn‚Äôt work well for non-linear models.\n",
        "\n",
        "Q19 How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "Ans-A large standard error suggests that the coefficient estimate is imprecise, meaning:\n",
        "\n",
        "There‚Äôs high variability in the coefficient across samples.\n",
        "\n",
        "The predictor might not be statistically significant.\n",
        "\n",
        "Could be a sign of multicollinearity or noise in data.\n",
        "\n",
        "Q20 - How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "Ans-Heteroscedasticity, or non-constant variance of errors, can be identified in residual plots by looking for a fan or cone shape, where the spread of residuals widens or narrows as the fitted values increase\n",
        "\n",
        "Q21 What does it mean if a Multiple Linear Regression model has a high R¬≤ but low adjusted R¬≤?\n",
        "\n",
        "Ans-the model explains a significant portion of the variance in the dependent variable, but the added predictors are not contributing much to the model's explanatory power, considering the number of variables used.\n",
        "\n",
        "Q22 Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Ans-to ensure that the model's coefficients are interpretable and that the model converges faster and more efficiently.\n",
        "\n",
        "Q23 What is polynomial regression?\n",
        "\n",
        "Ans-Polynomial regression is an extension of linear regression used to model non-linear relationships between variables.\n",
        "\n",
        "Q24 How does polynomial regression differ from linear regression?\n",
        "\n",
        "Ans-Polynomial regression extends linear regression by allowing for curved relationships between variables, whereas linear regression assumes a straight-line relationship.\n",
        "\n",
        "Q25 - When is polynomial regression used?\n",
        "\n",
        "Ans-Polynomial regression is used when the relationship between variables is non-linear and a straight line (as in linear regression) cannot accurately represent the data.\n",
        "\n",
        "Q26 What is the general equation for polynomial regression?\n",
        "\n",
        "Ans-The general equation for polynomial regression, representing the relationship between a dependent variable (y) and an independent variable (x) using an nth-degree polynomial, is:\n",
        "y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx¬≤ + Œ≤‚ÇÉx¬≥ + ... + Œ≤‚Çôx‚Åø + Œµ\n",
        "\n",
        "Q27 Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Ans-Yes, polynomial regression can be applied to multiple variables.\n",
        "\n",
        "Q28  What are the limitations of polynomial regression?\n",
        "\n",
        "Ans- It can be prone to overfitting, especially with high-degree polynomials, and can be computationally intensive.\n",
        "\n",
        "Q29 What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "Ans-Visual inspection, cross-validation, and information criteria like AIC and BIC.\n",
        "\n",
        "Q30 - Why is visualization important in polynomial regression?\n",
        "\n",
        "Ans-Visualization is crucial in polynomial regression for understanding the data's underlying pattern, assessing the appropriateness of a polynomial fit, and evaluating the model's performance.\n",
        "\n",
        "Q31  How is polynomial regression implemented in Python?\n",
        "\n",
        "Ans-Polynomial regression, implemented in Python, models non-linear relationships by adding polynomial terms to the independent variables. The scikit-learn library facilitates this process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TPCt5BTzwcw0"
      }
    }
  ]
}